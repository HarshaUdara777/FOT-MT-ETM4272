{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iMbGpO8c0c0P",
    "outputId": "55ccdde7-c06e-4cfc-af28-705a03012ca9"
   },
   "outputs": [],
   "source": [
    "!pip install supervision\n",
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ya_dJHmI1TGK",
    "outputId": "ffd26f4e-3578-4c6c-a6ee-70b9dc450eed"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import supervision as sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsEpdmxh1hag"
   },
   "source": [
    "3.1.1 Draw the segmentation mask of the largest car: Use ultralytics library only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rWJfw1S1jKs",
    "outputId": "fc9593c5-c6fa-4132-e365-7bef3f334937"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ashanW004/ETM4272.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDCkIAem2KYW"
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(\"/content/ETM4272/demo5_images/cars1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "p8kDALQ72lqC",
    "outputId": "0dea12df-3b0e-47ad-a80f-3fd03b0c4260"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8 segmentation model\n",
    "model = YOLO('yolov8x-seg.pt')  # Using the largest segmentation model\n",
    "\n",
    "# Load the image\n",
    "img = cv2.imread(\"/content/ETM4272/demo5_images/cars1.jpg\")\n",
    "image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Run segmentation on the image\n",
    "results = model(image)\n",
    "\n",
    "# Get segmentation masks\n",
    "masks = results[0].masks.xy  # List of masks (each mask is a set of polygon points)\n",
    "boxes = results[0].boxes.xyxy  # Bounding boxes\n",
    "\n",
    "# Find the largest car based on bounding box area\n",
    "largest_idx = None\n",
    "max_area = 0\n",
    "\n",
    "for i, box in enumerate(boxes):\n",
    "    x1, y1, x2, y2 = box\n",
    "    area = (x2 - x1) * (y2 - y1)\n",
    "    if area > max_area:\n",
    "        max_area = area\n",
    "        largest_idx = i\n",
    "\n",
    "# Draw the mask for the largest car\n",
    "if largest_idx is not None:\n",
    "    mask = np.array(masks[largest_idx], dtype=np.int32)\n",
    "\n",
    "    # Draw filled polygon mask on a blank image\n",
    "    mask_img = np.zeros_like(image)\n",
    "    cv2.fillPoly(mask_img, [mask], (255, 0, 0))  # Red mask for visualization\n",
    "\n",
    "    # Overlay mask on the original image\n",
    "    overlaid = cv2.addWeighted(image, 0.7, mask_img, 0.3, 0)\n",
    "\n",
    "    # Display the image with the largest car's mask\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(overlaid)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YM_i9jY3Vrl"
   },
   "source": [
    "3.2 Estimate distance to centroid of chair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4d06016b13c94d13aa1688fe63310949",
      "cf5081c3f9844fc1b60aa6552fd9a52b",
      "5646eb6a94ba4ae5bb321a0769b4220b",
      "8ddfabdaafde43339564784d6a4036b6",
      "6ca4765f91aa4b3f8133901da804341a",
      "9ac3d3ebbbd8411385d11c9513cfe6f5",
      "552429757a604f78881f380c72ffda15",
      "5e31c2a2121d4f3984a5f14aa7d594a9",
      "c5a3dca524ea4668b8e46b0d25069be1",
      "8d82cbfa66da4478ad69e1ab84d9e9aa",
      "427c6064271b4822b693bda1e86ad57c",
      "ed7b91f372554d00a560e6c684c9f811",
      "250213061e4f4a248bbbc1b3500df68a",
      "beee3420f3e64a838d874f891b07faac",
      "0237024964fb4a9bab9521c62abc340d",
      "d5ba5ddae55b419786802b21687eb334",
      "2315bc52f3ea4b96ba40e61037b2452f",
      "f0b5540426e24a40b37b7ccfeb977408",
      "0a2bb21a61f7453485f383e975870166",
      "6622b10156d6463bac79f6dbb79605de",
      "c889c325fb24442688ff75e335c86be2",
      "e6fddc74343a41fcb76ae9c58cb70b55",
      "d372ac6ba4394c4c9988db85a5f3282b",
      "0a7ea3646ef44fb2982b1382e1cf384b",
      "b06f6eda6d5e4e2eaace38c82c6c3ef3",
      "8e7562f1c3c54e138b2c3ed0138a2830",
      "36686ed143f449d5bf5c9bdb5448a7c4",
      "b9ec12a3aa36478f9a5498a98c2ec452",
      "aeb1784642454a6f8cdf56d613b9fff5",
      "ffa8cb71abc6413baf9f9a65978e7185",
      "c3f66a3c24064fedaf6786606255fd6a",
      "a98b036d6e4f488fbbf4d4ebe77cfcf1",
      "3d84fb64cd3b45c5ae0d85c30daaec86"
     ]
    },
    "id": "RCxP4Sl821wX",
    "outputId": "cc3c8753-786b-4300-9403-945df002174a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "from transformers import AutoProcessor\n",
    "from transformers import AutoModelForDepthEstimation\n",
    "\n",
    "# Load YOLOv8 model for object detection\n",
    "model = YOLO('yolov8x.pt')  # YOLOv8x for high accuracy\n",
    "\n",
    "# Load the image\n",
    "image_path = \"/content/ETM4272/demo5_images/kingChair.jpg\"  # Updated image path\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Perform object detection\n",
    "results = model(image)\n",
    "\n",
    "# Initialize centroid coordinates\n",
    "centroid_x, centroid_y = None, None\n",
    "\n",
    "# Extract bounding box of chair\n",
    "for result in results:\n",
    "    for box in result.boxes:\n",
    "        cls = int(box.cls[0])  # Class ID\n",
    "        if model.names[cls] == \"chair\":  # Check if detected object is a chair\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get bounding box coordinates\n",
    "            centroid_x = (x1 + x2) // 2\n",
    "            centroid_y = (y1 + y2) // 2\n",
    "            print(f\"Chair detected at: ({x1}, {y1}), ({x2}, {y2})\")\n",
    "            print(f\"Centroid: ({centroid_x}, {centroid_y})\")\n",
    "\n",
    "            # Draw bounding box and centroid\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.circle(image, (centroid_x, centroid_y), 5, (0, 0, 255), -1)\n",
    "\n",
    "# Save and display detected image\n",
    "detected_image_path = \"detected_chair.jpg\"\n",
    "cv2.imwrite(detected_image_path, image)\n",
    "\n",
    "# Show detected image\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Check if centroid was found\n",
    "if centroid_x is None or centroid_y is None:\n",
    "    print(\"No chair detected!\")\n",
    "else:\n",
    "    # Load Depth Anything V2 model\n",
    "    # Load Depth Anything V2 model\n",
    "    processor = AutoProcessor.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "    depth_model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "\n",
    "    # Convert image to PIL format\n",
    "    image_pil = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Get depth map\n",
    "    inputs = processor(image_pil, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        depth_output = depth_model(**inputs).predicted_depth\n",
    "\n",
    "    # Convert depth map to NumPy array\n",
    "    depth_map = depth_output.squeeze().cpu().numpy()\n",
    "\n",
    "    # Show depth map\n",
    "    plt.imshow(depth_map, cmap=\"plasma\")\n",
    "    plt.colorbar(label=\"Depth Value\")\n",
    "    plt.scatter(centroid_x, centroid_y, color=\"red\", marker=\"x\", label=\"Centroid\")\n",
    "    plt.legend()\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # Query depth value at centroid\n",
    "    depth_value = depth_map[centroid_y, centroid_x]\n",
    "    print(f\"Depth at centroid ({centroid_x}, {centroid_y}): {depth_value}\")\n",
    "    # Display results\n",
    "    print(f\"Estimated Distance to Chair: {depth_value:.2f} meters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFjJ9HhT4TK8"
   },
   "source": [
    "3.3.1 Draw bounding boxes around the \"cars\" using supervision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "as92iVIY4nfy",
    "outputId": "0f011068-6fa9-40cb-865d-905c9059835b"
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "url = \"https://drive.google.com/uc?id=1zcKvnDDEdyFF4B0B3eYud6DHU19nl0o4\"\n",
    "output = \"vehicles_video.mp4\"\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_d14WuMg4wVf",
    "outputId": "8355c772-9f4b-40ac-b463-9c877c08d68b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import supervision as sv\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(\"yolo11n.pt\")  # Ensure the model file is present\n",
    "\n",
    "# Define a BoxAnnotator instance (for drawing bounding boxes)\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "\n",
    "def callback(frame: np.ndarray, _: int) -> np.ndarray:\n",
    "    # Run YOLO on the frame\n",
    "    results = model(frame)\n",
    "\n",
    "    # Convert results to Supervision format\n",
    "    detections = sv.Detections.from_ultralytics(results[0])\n",
    "\n",
    "    # Annotate frame using the BoxAnnotator\n",
    "    annotated_frame = box_annotator.annotate(scene=frame, detections=detections)\n",
    "\n",
    "    return annotated_frame\n",
    "\n",
    "# Process the video\n",
    "sv.process_video(\n",
    "    source_path=\"vehicles_video.mp4\",\n",
    "    target_path=\"result_tracking.mp4\",\n",
    "    callback=callback\n",
    ")\n",
    "\n",
    "print(\"Processing complete. Check 'result_tracking.mp4'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AvooHsr5qnb"
   },
   "source": [
    "3.3.1 Track the cars. Add the tracker id to the bounding boxes around the cars. Use supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TvWMR7w15xlw",
    "outputId": "8cd8a5ed-1678-4a00-b04d-09b31c9119b4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import supervision as sv\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(\"yolov8n.pt\")  # Ensure this model exists\n",
    "\n",
    "# Create a ByteTrack tracker\n",
    "tracker = sv.ByteTrack()\n",
    "\n",
    "# Define the BoxAnnotator\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "\n",
    "def callback(frame: np.ndarray, _: int) -> np.ndarray:\n",
    "    # Run YOLO on the frame\n",
    "    results = model(frame)\n",
    "\n",
    "    # Convert YOLO results to Supervision detections\n",
    "    detections = sv.Detections.from_ultralytics(results[0])\n",
    "\n",
    "    # Update tracker with new detections\n",
    "    detections = tracker.update_with_detections(detections)\n",
    "\n",
    "    # Generate labels with tracker ID\n",
    "    labels = [f\"ID {round(tracker_id)}\" for tracker_id in detections.tracker_id]\n",
    "\n",
    "    # Annotate frame with bounding boxes and tracker IDs\n",
    "    annotated_frame = box_annotator.annotate(scene=frame, detections=detections)\n",
    "\n",
    "    # Add labels manually using Supervision's LabelAnnotator\n",
    "    label_annotator = sv.LabelAnnotator()\n",
    "    annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
    "\n",
    "    return annotated_frame\n",
    "\n",
    "# Process the video\n",
    "sv.process_video(\n",
    "    source_path=\"vehicles_video.mp4\",\n",
    "    target_path=\"tracked_cars.mp4\",\n",
    "    callback=callback\n",
    ")\n",
    "\n",
    "print(\"✅ Processing complete! Check 'tracked_cars.mp4'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQeC8Dtw6ZMm"
   },
   "source": [
    "3.3.2 Draw the track traces for the tracked cars. Use supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ahkuIvOs6eUz",
    "outputId": "630df0b2-ddcd-425c-e720-2761a44ec20c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import supervision as sv\n",
    "from ultralytics import YOLO\n",
    "import cv2  # OpenCV for drawing traces\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(\"yolov8n.pt\")  # Ensure this model exists\n",
    "\n",
    "# Create a ByteTrack tracker\n",
    "tracker = sv.ByteTrack()\n",
    "\n",
    "# Define the BoxAnnotator\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "\n",
    "# Dictionary to store car paths (each car's tracked positions)\n",
    "car_paths = {}\n",
    "\n",
    "def callback(frame: np.ndarray, _: int) -> np.ndarray:\n",
    "    # Run YOLO on the frame\n",
    "    results = model(frame)\n",
    "\n",
    "    # Convert YOLO results to Supervision detections\n",
    "    detections = sv.Detections.from_ultralytics(results[0])\n",
    "\n",
    "    # Update tracker with new detections\n",
    "    detections = tracker.update_with_detections(detections)\n",
    "\n",
    "    # Generate labels with rounded tracker ID\n",
    "    labels = [f\"ID {round(tracker_id)}\" for tracker_id in detections.tracker_id]\n",
    "\n",
    "    # Track the positions of each car over time\n",
    "    for tracker_id, bbox in zip(detections.tracker_id, detections.xyxy):  # Use detections.xyxy\n",
    "        if tracker_id not in car_paths:\n",
    "            car_paths[tracker_id] = []  # Initialize path for this car\n",
    "        # Save the current position (center of the bounding box)\n",
    "        center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)\n",
    "        car_paths[tracker_id].append(center)\n",
    "\n",
    "    # Draw traces for each car\n",
    "    for tracker_id, path in car_paths.items():\n",
    "        # Ensure there are at least two points to draw a line\n",
    "        if len(path) > 1:\n",
    "            for i in range(1, len(path)):\n",
    "                cv2.line(frame, tuple(map(int, path[i - 1])), tuple(map(int, path[i])), (0, 255, 0), 2)\n",
    "\n",
    "    # Annotate frame with bounding boxes and tracker IDs\n",
    "    annotated_frame = box_annotator.annotate(scene=frame, detections=detections)\n",
    "\n",
    "    # Add labels manually using Supervision's LabelAnnotator\n",
    "    label_annotator = sv.LabelAnnotator()\n",
    "    annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
    "\n",
    "    return annotated_frame\n",
    "\n",
    "# Process the video\n",
    "sv.process_video(\n",
    "    source_path=\"vehicles_video.mp4\",\n",
    "    target_path=\"tracked_cars_with_traces.mp4\",\n",
    "    callback=callback\n",
    ")\n",
    "\n",
    "print(\"✅ Processing complete! Check 'tracked_cars_with_traces.mp4'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ur-GAEtc69Xh"
   },
   "source": [
    "Questions:\n",
    "\n",
    "Explain what \"def callback(frame: np.ndarray, _: int) -> np.ndarray:\" does.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The function definition def callback(frame: np.ndarray, _: int) -> np.ndarray: declares a callback function designed to process a numpy array (typically an image/video frame) and return a modified numpy array. Here's a breakdown:\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "Parameters:\n",
    "\n",
    "frame: np.ndarray:\n",
    "A numpy array representing input data (e.g., an image or video frame).\n",
    "\n",
    "_: int:\n",
    "An integer argument (conventionally named _ to indicate it is intentionally ignored/unused by the function).\n",
    "\n",
    "Return Type:\n",
    "\n",
    "-> np.ndarray:\n",
    "The function returns a processed/modified numpy array (e.g., a filtered or transformed version of frame).\n",
    "\n",
    "Purpose:\n",
    "This function is typically passed to another system (e.g., a video processing pipeline) that invokes it repeatedly (e.g., for each frame in a video stream).\n",
    "\n",
    "The second parameter (often a frame index, timestamp, or unused flag) is included for API compatibility but not used in the function's logic.\n",
    "\n",
    "**Completed section**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
